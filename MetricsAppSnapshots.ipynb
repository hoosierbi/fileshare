{"cells":[{"cell_type":"code","source":["# Load need packages and provide names\n","\n","!pip install semantic-link --q\n"," \n","import pandas as pd\n","import sempy.fabric as fabric\n","from datetime import datetime,date,timedelta\n","\n","MetricsWS = \"MahoneyMetricsApp_Mar2024\"\n","MetricsModel = \"Fabric Capacity Metrics\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"1341aaa1-20c9-4f64-aa33-9db5f38a6fc7","statement_id":3,"state":"finished","livy_statement_state":"available","queued_time":"2024-03-09T15:42:58.2259504Z","session_start_time":"2024-03-09T15:42:58.4693644Z","execution_start_time":"2024-03-09T15:43:10.4549437Z","execution_finish_time":"2024-03-09T15:43:33.3613991Z","parent_msg_id":"a2d79746-8355-4a9d-b164-745c3fc08ebf"},"text/plain":"StatementMeta(, 1341aaa1-20c9-4f64-aa33-9db5f38a6fc7, 3, Finished, Available)"},"metadata":{}}],"execution_count":1,"metadata":{},"id":"007f42f2-304a-4079-a451-f5da494a743c"},{"cell_type":"code","source":["# Get Capacities, write to daily csv, overwrite Capacities Table\n","mssparkutils.fs.mkdirs(\"Files/Capacities/\")\n","df_capacities = fabric.evaluate_dax(workspace=MetricsWS, dataset=MetricsModel, dax_string=\"EVALUATE Capacities\")\n","df_capacities.columns = df_capacities.columns.str.replace(r'(.*\\[)|(\\].*)', '', regex=True)\n","df_capacities.columns = df_capacities.columns.str.replace(' ', '_')\n","#Need CU for each SKU to enable utilization calculation\n","skus = {'SKU': ['P1', 'P2', 'P3', 'P4', 'P5', 'F2', 'F4', 'F8', 'F16', 'F32', 'F64', 'F128', 'F256', 'F512', 'F1024', 'F2048', 'FT1', 'A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'DCT1', 'EM1', 'EM2', 'EM3'], 'CUperSecond': [64, 128, 256, 512, 1024, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 64, 8, 16, 32, 64, 128, 256, 512, 1024, 64, 8, 16, 32], 'CUperHour': [230400, 460800, 921600, 1843200, 3686400, 7200, 14400, 28800, 57600, 115200, 230400, 460800, 921600, 1843200, 3686400, 7372800, 230400, 28800, 57600, 115200, 230400, 460800, 921600, 1843200, 3686400, 230400, 28800, 57600, 115200]}\n","df_skus = pd.DataFrame(skus)\n","df_capacities = df_capacities.merge(df_skus, left_on=\"sku\", right_on=\"SKU\")\n","df_capacities = df_capacities.drop('SKU', axis=1)\n","filename = 'Capacities_' + (datetime.today()).strftime('%Y%m%d') + '.csv'\n","df_capacities.to_csv(\"/lakehouse/default/Files/Capacities/\" + filename)\n","spk_capacities = spark.createDataFrame(df_capacities)\n","spk_capacities.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable('Capacities')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"40e78a44-2b91-4275-9feb-876b3ec1c2a2","statement_id":4,"state":"finished","livy_statement_state":"available","queued_time":"2024-03-09T12:51:39.4190606Z","session_start_time":null,"execution_start_time":"2024-03-09T12:51:39.9052205Z","execution_finish_time":"2024-03-09T12:52:18.798592Z","parent_msg_id":"8c0cf0af-3a20-4481-b7f9-3973f060ec1f"},"text/plain":"StatementMeta(, 40e78a44-2b91-4275-9feb-876b3ec1c2a2, 4, Finished, Available)"},"metadata":{}}],"execution_count":2,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"7459c7e9-55a0-4ee5-8a92-b75f78ac3dff"},{"cell_type":"code","source":["# Get Items, write to daily csv, overwrite Items Table\n","mssparkutils.fs.mkdirs(\"Files/Items/\")\n","df_items = fabric.evaluate_dax(workspace=MetricsWS, dataset=MetricsModel, dax_string=\"EVALUATE Items\")\n","df_items.columns = df_items.columns.str.replace(r'(.*\\[)|(\\].*)', '', regex=True)\n","df_items.columns = df_items.columns.str.replace(' ', '_')\n","df_items.drop_duplicates(subset=['ItemId'], keep='first', inplace=True)\n","filename = 'Items_' + (datetime.today()).strftime('%Y%m%d') + '.csv'\n","df_items.to_csv(\"/lakehouse/default/Files/Items/\" + filename)\n","spk_items = spark.createDataFrame(df_items)\n","spk_items.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable('Items')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"1341aaa1-20c9-4f64-aa33-9db5f38a6fc7","statement_id":7,"state":"finished","livy_statement_state":"available","queued_time":"2024-03-09T15:56:40.1420677Z","session_start_time":null,"execution_start_time":"2024-03-09T15:56:40.6534886Z","execution_finish_time":"2024-03-09T15:57:08.3281784Z","parent_msg_id":"098ae87e-9b75-4f5c-a50b-920456d30cfd"},"text/plain":"StatementMeta(, 1341aaa1-20c9-4f64-aa33-9db5f38a6fc7, 7, Finished, Available)"},"metadata":{}}],"execution_count":5,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"0f167274-1fa7-4fe5-b084-062ecfe11594"},{"cell_type":"code","source":["# UsageData\n","# Get list of capacities\n","lst_capacities = df_capacities['capacityId'].tolist()\n","\n","# Create dir if doesn't exist\n","mssparkutils.fs.mkdirs(\"Files/UsageData/\")\n","\n","# Get max date from current delta table (to avoid loading duplicate days)\n","try:\n","    df_max = spark.sql(f'''\n","    SELECT MAX(Date) as MaxDate\n","    FROM usage;\n","    '''\n","    )\n","    maxdate = df_max.first()['MaxDate']\n","except:\n","    maxdate = datetime.today() + timedelta(days=-6)\n","    #Note - pulling too much data may result in 64 Mb error; reduce number of days for initial load\n","maxdateforDAX = maxdate.strftime('%Y,%m,%d')\n","\n","if maxdate.date() < (datetime.today() + timedelta(days=-1)).date():\n","\n","    # Get data for each capacity, write daily csv and append delta\n","    for capacity in lst_capacities:   \n","        querytext = '''\\\n","                    DEFINE MPARAMETER 'CapacityID' =  \"{capID}\"\n","                    var Yesterday = TODAY() - 1\n","\n","                    EVALUATE\n","                    CALCULATETABLE(MetricsByItemandOperationandHour, MetricsByItemandOperationandHour[Date] <= Yesterday && MetricsByItemandOperationandHour[Date] > DATE({MD}))\n","                    '''.format(capID=capacity, MD=maxdateforDAX)\n","        df_hourly = fabric.evaluate_dax(workspace=MetricsWS, dataset=MetricsModel, dax_string=querytext)\n","        if len(df_hourly) >= 1:\n","            df_hourly.columns = df_hourly.columns.str.replace(r'(.*\\[)|(\\].*)', '', regex=True)\n","            df_hourly.columns = df_hourly.columns.str.replace(' ', '_')\n","            df_hourly.rename(columns={'Throttling_(min)': 'ThrottlingMin'}, inplace=True)\n","            filename = capacity + '_Usage_' + (datetime.today()).strftime('%Y%m%d') + '.csv'\n","            df_hourly.to_csv(\"/lakehouse/default/Files/UsageData/\" + filename)\n","            spk_usage = spark.createDataFrame(df_hourly)\n","            spk_usage.write.mode(\"append\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable('Usage')\n","else:\n","    print(\"Data already loaded\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"40e78a44-2b91-4275-9feb-876b3ec1c2a2","statement_id":7,"state":"finished","livy_statement_state":"available","queued_time":"2024-03-09T13:02:09.0087075Z","session_start_time":null,"execution_start_time":"2024-03-09T13:02:09.8972311Z","execution_finish_time":"2024-03-09T13:03:06.3152161Z","parent_msg_id":"65a4b29c-0de2-471a-b201-23dfccad569c"},"text/plain":"StatementMeta(, 40e78a44-2b91-4275-9feb-876b3ec1c2a2, 7, Finished, Available)"},"metadata":{}}],"execution_count":5,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"f147c831-edcc-46b1-8fec-fc72f204590c"},{"cell_type":"code","source":["# ThrottlingData\n","# Get list of capacities\n","lst_capacities = df_capacities['capacityId'].tolist()\n","\n","# Create dir if doesn't exist\n","mssparkutils.fs.mkdirs(\"Files/ThrottlingData/\")\n","\n","# Get max date from current delta table (to avoid loading duplicate days)\n","try:\n","    df_max = spark.sql(f'''\n","    SELECT MAX(Date) as MaxDate\n","    FROM throttling;\n","    '''\n","    )\n","    maxdate = df_max.first()['MaxDate']\n","except:\n","    maxdate = datetime.today() + timedelta(days=-6)\n","maxdateforDAX = maxdate.strftime('%Y,%m,%d')\n","\n","if maxdate.date() < (datetime.today() + timedelta(days=-1)).date():\n","\n","    # Get data for each capacity, write daily csv and append delta\n","    for capacity in lst_capacities:   \n","        querytext = '''\\\n","                    DEFINE\n","                    MPARAMETER 'CapacityID' = \"{capID}\"\n","                    VAR yesterday =\n","                        FILTER(ALL('Dates'[Date] ), 'Dates'[Date] < TODAY() && 'Dates'[Date] > DATE({MD}) )\n","\n","                    EVALUATE\n","                    SUMMARIZECOLUMNS(\n","                        'Dates'[Date],\n","                        'TimePoints'[Start of Hour],\n","                        yesterday,\n","                        \"IntDelay\", ROUND( 'All Measures'[Dynamic InteractiveDelay %] * 100, 2 ),\n","                        \"IntReject\", ROUND( 'All Measures'[Dynamic InteractiveRejection %] * 100, 2 ),\n","                        \"BackReject\", ROUND( 'All Measures'[Dynamic BackgroundRejection %] * 100, 2 )\n","                    )\n","                    '''.format(capID=capacity, MD=maxdateforDAX)\n","        df_throttling = fabric.evaluate_dax(workspace=MetricsWS, dataset=MetricsModel, dax_string=querytext)\n","        if len(df_throttling) >= 1:\n","            df_throttling.columns = df_throttling.columns.str.replace(r'(.*\\[)|(\\].*)', '', regex=True)\n","            df_throttling.columns = df_throttling.columns.str.replace(' ', '_')\n","            df_throttling['capacityId'] = capacity\n","            filename = capacity + '_throttling_' + (datetime.today()).strftime('%Y%m%d') + '.csv'\n","            df_throttling.to_csv(\"/lakehouse/default/Files/ThrottlingData/\" + filename)\n","            spk_throttle = spark.createDataFrame(df_throttling)\n","            spk_throttle.write.mode(\"append\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable('Throttling')\n","\n","else:\n","    print(\"Data already loaded\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"40e78a44-2b91-4275-9feb-876b3ec1c2a2","statement_id":8,"state":"finished","livy_statement_state":"available","queued_time":"2024-03-09T13:13:38.5115723Z","session_start_time":null,"execution_start_time":"2024-03-09T13:13:38.9985101Z","execution_finish_time":"2024-03-09T13:14:08.8267551Z","parent_msg_id":"fc39fd3d-f54a-4c6c-ac7f-c7e9aa06cc48"},"text/plain":"StatementMeta(, 40e78a44-2b91-4275-9feb-876b3ec1c2a2, 8, Finished, Available)"},"metadata":{}}],"execution_count":6,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"3f452c05-2fca-4fdf-8115-6076f9a0938d"},{"cell_type":"code","source":["# DateHour Table\n","import pandas as pd\n","from datetime import datetime\n","import numpy as np\n","import math\n","from pandas.tseries.offsets import MonthEnd, QuarterEnd\n","\n","# Create a list of dates from 2020 to 2024\n","start_date = datetime(2024, 2, 24)\n","end_date = pd.to_datetime(\"today\") #datetime(2024, 12, 31)\n","dates = pd.date_range(start=start_date, end=end_date, freq = '1H')\n","\n","# Create a dataframe with the desired columns\n","df = pd.DataFrame({\n","    'DateHour': dates,\n","    'Hour': dates.strftime('%H').astype('int'),\n","    'Year': dates.year,\n","    'MonthNumber': dates.month,\n","    'Month': dates.strftime('%B'),\n","    'MonthShort': dates.strftime('%b'),\n","    'Day': dates.strftime('%A').astype('str'),\n","    'DayShort': dates.strftime('%a'),\n","    'DayOfWeek': dates.weekday,\n","    'DayOfYear': dates.strftime('%j').astype('int'),\n","    'WeekOfYear': dates.strftime('%W').astype('int'),\n","    'YearQuarter': dates.year.astype('str') + \"Q\" + dates.quarter.astype('str'),\n","    'YearWeek': dates.strftime('%Y%W'),\n","    'EOM': pd.to_datetime(dates, format=\"%Y%m\") + MonthEnd(0),\n","    'EOQ': pd.to_datetime(dates, format=\"%Y%m\") + QuarterEnd(0)\n","})\n","\n","# Add date column\n","df['Date'] = pd.to_datetime(pd.to_datetime(df['DateHour']).dt.date)\n","\n","# Create index columns from today\n","today = datetime.today()\n","df['DaysFromToday'] = (df['Date']-today).dt.days\n","df['WeeksFromToday'] = ((df['Date']-today).dt.days/7).astype('int')\n","df['MonthsFromToday'] = ((df['Year']-today.year) * 12 + ( df['MonthNumber']-today.month))\n","df['QtrsFromToday'] = (df['MonthsFromToday'].astype('int')/3).astype('int')\n","df['YearsFromToday'] = df['Year']-today.year\n","\n","# Add Is Working Day\n","\n","df['WorkingDay'] = np.where((df['DayOfWeek'] != 5) & (df['DayOfWeek'] != 6), True, False)\n","\n","# Define function to add zero-width spaces for month and day names\n","def AddZWS(row, col, num):\n","    return (num-row[col]) * chr(8203)\n","\n","# Add ZWS columns for month and day\n","df['MonthZWS'] = df.apply(AddZWS, num=12, col='MonthNumber', axis=1) + df['Month']\n","df['DayZWS'] = df.apply(AddZWS, num=6, col='DayOfWeek', axis=1) + df['Day']\n","\n","# Convert pandas DF to spark DF and write to delta\n","# Remove spaces from column names (if any)\n","df.columns = df.columns.str.replace(' ', '')\n","\n","# Create Spark Datafram and Load to Delta Table\n","sparkDF = spark.createDataFrame(df)\n","sparkDF.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable(\"DateHour\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"489ca10a-bceb-41e0-81ae-b3bed6584be7","statement_id":62,"state":"finished","livy_statement_state":"available","queued_time":"2024-03-09T01:11:05.7530242Z","session_start_time":null,"execution_start_time":"2024-03-09T01:11:06.1324412Z","execution_finish_time":"2024-03-09T01:11:12.5832319Z","parent_msg_id":"8502f165-d590-47d5-b818-4b632665e4cb"},"text/plain":"StatementMeta(, 489ca10a-bceb-41e0-81ae-b3bed6584be7, 62, Finished, Available)"},"metadata":{}}],"execution_count":60,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"220da884-dc67-4b48-b233-d76c7c117bc2"},{"cell_type":"code","source":["# Measures Table\n","# Run this one time only, then comment it out\n","# import pandas as pd\n","\n","# data = {'Number':  [1,2,3] }\n","# df = pd.DataFrame(data)\n","# sparkDF = spark.createDataFrame(df)\n","# sparkDF.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable(\"measures_\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"122900b8-bfd3-4cb5-886a-426105e1a976","statement_id":9,"state":"finished","livy_statement_state":"available","queued_time":"2024-03-08T12:57:48.0099138Z","session_start_time":null,"execution_start_time":"2024-03-08T13:00:42.8502032Z","execution_finish_time":"2024-03-08T13:00:43.0828186Z","parent_msg_id":"25783cf9-2b12-44d5-8731-b0fc922553bb"},"text/plain":"StatementMeta(, 122900b8-bfd3-4cb5-886a-426105e1a976, 9, Finished, Available)"},"metadata":{}}],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"dcfd824e-1e0c-4745-8fe6-f1a05c9e2c8d"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"microsoft":{"language":"python"},"widgets":{},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"trident":{"lakehouse":{"default_lakehouse":"8a5f6589-2a5f-47bf-8605-0d1a2c6c3703","default_lakehouse_name":"MetricsSnapshotsLH","default_lakehouse_workspace_id":"8e216414-85e7-4330-99e1-ea3d9a3fd17d","known_lakehouses":[{"id":"65d3170d-458b-4cbb-be0e-e62d8cb3e943"},{"id":"8a5f6589-2a5f-47bf-8605-0d1a2c6c3703"}]}}},"nbformat":4,"nbformat_minor":5}