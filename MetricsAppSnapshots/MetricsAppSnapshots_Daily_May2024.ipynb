{"cells":[{"cell_type":"code","source":["# Load need packages and provide Metrics App names\n","\n","!pip install semantic-link --q\n"," \n","import pandas as pd\n","import sempy.fabric as fabric\n","from datetime import datetime,date,timedelta\n","\n","#Update with names from your install of the Fabric Metrics Apps\n","#Note - This code works with V1.5 of Metrics App (March 2024). Breaking changes possible in later updates.\n","MetricsWS = \"MetricsApp_May2024\"\n","MetricsModel = \"Fabric Capacity Metrics\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"956193d3-6680-4ed6-ae44-6c3174572e3f","statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","queued_time":"2024-05-14T12:01:53.6513496Z","session_start_time":"2024-05-14T12:01:53.9791697Z","execution_start_time":"2024-05-14T12:02:05.8581138Z","execution_finish_time":"2024-05-14T12:02:31.2739915Z","parent_msg_id":"6e123600-ee1d-4fb3-8c09-2f019f6bcf9f"},"text/plain":"StatementMeta(, 956193d3-6680-4ed6-ae44-6c3174572e3f, 3, Finished, Available)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"007f42f2-304a-4079-a451-f5da494a743c"},{"cell_type":"code","source":["# Get Capacities, write to daily csv, overwrite Capacities Table\n","mssparkutils.fs.mkdirs(\"Files/Capacities/\")\n","df_capacities = fabric.evaluate_dax(workspace=MetricsWS, dataset=MetricsModel, dax_string=\"EVALUATE Capacities\")\n","df_capacities.columns = df_capacities.columns.str.replace(r'(.*\\[)|(\\].*)', '', regex=True)\n","df_capacities.columns = df_capacities.columns.str.replace(' ', '_')\n","#Need CU for each SKU to enable utilization calculation\n","skus = {'SKU': ['P1', 'P2', 'P3', 'P4', 'P5', 'F2', 'F4', 'F8', 'F16', 'F32', 'F64', 'F128', 'F256', 'F512', 'F1024', 'F2048', 'FT1', 'A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'DCT1', 'EM1', 'EM2', 'EM3'], 'CUperSecond': [64, 128, 256, 512, 1024, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 64, 8, 16, 32, 64, 128, 256, 512, 1024, 64, 8, 16, 32], 'CUperHour': [230400, 460800, 921600, 1843200, 3686400, 7200, 14400, 28800, 57600, 115200, 230400, 460800, 921600, 1843200, 3686400, 7372800, 230400, 28800, 57600, 115200, 230400, 460800, 921600, 1843200, 3686400, 230400, 28800, 57600, 115200]}\n","df_skus = pd.DataFrame(skus)\n","df_capacities = df_capacities.merge(df_skus, left_on=\"sku\", right_on=\"SKU\")\n","df_capacities = df_capacities.drop('SKU', axis=1)\n","filename = 'Capacities_' + (datetime.today()).strftime('%Y%m%d') + '.csv'\n","df_capacities.to_csv(\"/lakehouse/default/Files/Capacities/\" + filename)\n","spk_capacities = spark.createDataFrame(df_capacities)\n","spk_capacities.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable('Capacities')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"956193d3-6680-4ed6-ae44-6c3174572e3f","statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","queued_time":"2024-05-14T12:02:47.8455988Z","session_start_time":null,"execution_start_time":"2024-05-14T12:02:48.3209797Z","execution_finish_time":"2024-05-14T12:03:16.0479197Z","parent_msg_id":"7e286019-8f56-4341-92d6-f6a182e3450d"},"text/plain":"StatementMeta(, 956193d3-6680-4ed6-ae44-6c3174572e3f, 4, Finished, Available)"},"metadata":{}}],"execution_count":2,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7459c7e9-55a0-4ee5-8a92-b75f78ac3dff"},{"cell_type":"code","source":["# Get Items, write to daily csv, overwrite Items Table\n","mssparkutils.fs.mkdirs(\"Files/Items/\")\n","df_items = fabric.evaluate_dax(workspace=MetricsWS, dataset=MetricsModel, dax_string=\"EVALUATE Items\")\n","df_items.columns = df_items.columns.str.replace(r'(.*\\[)|(\\].*)', '', regex=True)\n","df_items.columns = df_items.columns.str.replace(' ', '_')\n","df_items.drop_duplicates(subset=['ItemId'], keep='first', inplace=True)\n","filename = 'Items_' + (datetime.today()).strftime('%Y%m%d') + '.csv'\n","df_items.to_csv(\"/lakehouse/default/Files/Items/\" + filename)\n","spk_items = spark.createDataFrame(df_items)\n","spk_items.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable('Items')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"956193d3-6680-4ed6-ae44-6c3174572e3f","statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","queued_time":"2024-05-14T12:03:23.5518336Z","session_start_time":null,"execution_start_time":"2024-05-14T12:03:24.6208938Z","execution_finish_time":"2024-05-14T12:03:34.6936785Z","parent_msg_id":"6c039147-99c6-434a-baf3-19329ed91f88"},"text/plain":"StatementMeta(, 956193d3-6680-4ed6-ae44-6c3174572e3f, 5, Finished, Available)"},"metadata":{}}],"execution_count":3,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0f167274-1fa7-4fe5-b084-062ecfe11594"},{"cell_type":"code","source":["# UsageData\n","# Get list of capacities\n","lst_capacities = df_capacities['capacityId'].tolist()\n","\n","# Create dir if doesn't exist\n","mssparkutils.fs.mkdirs(\"Files/UsageData/\")\n","\n","# Get max date from current delta table (to avoid loading duplicate days)\n","try:\n","    df_max = spark.sql(f'''\n","    SELECT MAX(Date) as MaxDate\n","    FROM usage;\n","    '''\n","    )\n","    maxdate = df_max.first()['MaxDate']\n","except:\n","    maxdate = datetime.today() + timedelta(days=-6)\n","    #Note - pulling too much data may result in 64 Mb error; reduce number of days for initial load\n","maxdateforDAX = maxdate.strftime('%Y,%m,%d')\n","\n","if maxdate.date() < (datetime.today() + timedelta(days=-1)).date():\n","\n","    # Get data for each capacity, write daily csv and append delta\n","    for capacity in lst_capacities:   \n","        querytext = '''\\\n","                    DEFINE MPARAMETER 'CapacityID' =  \"{capID}\"\n","                    var Yesterday = TODAY() - 1\n","\n","                    EVALUATE\n","                    CALCULATETABLE(MetricsByItemandOperationandHour, MetricsByItemandOperationandHour[Date] <= Yesterday && MetricsByItemandOperationandHour[Date] > DATE({MD}))\n","                    '''.format(capID=capacity, MD=maxdateforDAX)\n","        df_hourly = fabric.evaluate_dax(workspace=MetricsWS, dataset=MetricsModel, dax_string=querytext)\n","        if len(df_hourly) >= 1:\n","            df_hourly.columns = df_hourly.columns.str.replace(r'(.*\\[)|(\\].*)', '', regex=True)\n","            df_hourly.columns = df_hourly.columns.str.replace(' ', '_')\n","            df_hourly.rename(columns={'Throttling_(min)': 'ThrottlingMin'}, inplace=True)\n","            filename = capacity + '_Usage_' + (datetime.today()).strftime('%Y%m%d') + '.csv'\n","            df_hourly.to_csv(\"/lakehouse/default/Files/UsageData/\" + filename)\n","            spk_usage = spark.createDataFrame(df_hourly)\n","            spk_usage.write.mode(\"append\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable('Usage')\n","else:\n","    print(\"Data already loaded\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"956193d3-6680-4ed6-ae44-6c3174572e3f","statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","queued_time":"2024-05-14T12:03:23.6253392Z","session_start_time":null,"execution_start_time":"2024-05-14T12:03:35.1485415Z","execution_finish_time":"2024-05-14T12:04:29.5639714Z","parent_msg_id":"58ab990f-4057-4610-89f3-de880f4483d9"},"text/plain":"StatementMeta(, 956193d3-6680-4ed6-ae44-6c3174572e3f, 6, Finished, Available)"},"metadata":{}}],"execution_count":4,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f147c831-edcc-46b1-8fec-fc72f204590c"},{"cell_type":"code","source":["# ThrottlingData\n","# Get list of capacities\n","lst_capacities = df_capacities['capacityId'].tolist()\n","\n","# Create dir if doesn't exist\n","mssparkutils.fs.mkdirs(\"Files/ThrottlingData/\")\n","\n","# Get max date from current delta table (to avoid loading duplicate days)\n","try:\n","    df_max = spark.sql(f'''\n","    SELECT MAX(Date) as MaxDate\n","    FROM throttling;\n","    '''\n","    )\n","    maxdate = df_max.first()['MaxDate']\n","except:\n","    maxdate = datetime.today() + timedelta(days=-6)\n","maxdateforDAX = maxdate.strftime('%Y,%m,%d')\n","\n","if maxdate.date() < (datetime.today() + timedelta(days=-1)).date():\n","\n","    # Get data for each capacity, write daily csv and append delta\n","    for capacity in lst_capacities:   \n","        querytext = '''\\\n","                    DEFINE\n","                    MPARAMETER 'CapacityID' = \"{capID}\"\n","                    VAR yesterday =\n","                        FILTER(ALL('Dates'[Date] ), 'Dates'[Date] < TODAY() && 'Dates'[Date] > DATE({MD}) )\n","\n","                    EVALUATE\n","                    SUMMARIZECOLUMNS(\n","                        'Dates'[Date],\n","                        'TimePoints'[Start of Hour],\n","                        yesterday,\n","                        \"IntDelay\", ROUND( 'All Measures'[Dynamic InteractiveDelay %] * 100, 2 ),\n","                        \"IntReject\", ROUND( 'All Measures'[Dynamic InteractiveRejection %] * 100, 2 ),\n","                        \"BackReject\", ROUND( 'All Measures'[Dynamic BackgroundRejection %] * 100, 2 )\n","                    )\n","                    '''.format(capID=capacity, MD=maxdateforDAX)\n","        df_throttling = fabric.evaluate_dax(workspace=MetricsWS, dataset=MetricsModel, dax_string=querytext)\n","        if len(df_throttling) >= 1:\n","            df_throttling.columns = df_throttling.columns.str.replace(r'(.*\\[)|(\\].*)', '', regex=True)\n","            df_throttling.columns = df_throttling.columns.str.replace(' ', '_')\n","            df_throttling['capacityId'] = capacity\n","            filename = capacity + '_throttling_' + (datetime.today()).strftime('%Y%m%d') + '.csv'\n","            df_throttling.to_csv(\"/lakehouse/default/Files/ThrottlingData/\" + filename)\n","            spk_throttle = spark.createDataFrame(df_throttling)\n","            spk_throttle.write.mode(\"append\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable('Throttling')\n","\n","else:\n","    print(\"Data already loaded\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"956193d3-6680-4ed6-ae44-6c3174572e3f","statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","queued_time":"2024-05-14T12:03:23.7039806Z","session_start_time":null,"execution_start_time":"2024-05-14T12:04:30.0389892Z","execution_finish_time":"2024-05-14T12:04:53.6436232Z","parent_msg_id":"498d26c8-3d28-41fb-84b4-c30f5bdfeab8"},"text/plain":"StatementMeta(, 956193d3-6680-4ed6-ae44-6c3174572e3f, 7, Finished, Available)"},"metadata":{}}],"execution_count":5,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3f452c05-2fca-4fdf-8115-6076f9a0938d"},{"cell_type":"code","source":["# DateHour Table\n","import pandas as pd\n","from datetime import datetime\n","import numpy as np\n","import math\n","from pandas.tseries.offsets import MonthEnd, QuarterEnd\n","\n","# Create a list of dates from 2020 to 2024\n","start_date = datetime(2024, 2, 24)\n","end_date = pd.to_datetime(\"today\") #datetime(2024, 12, 31)\n","dates = pd.date_range(start=start_date, end=end_date, freq = '1H')\n","\n","# Create a dataframe with the desired columns\n","df = pd.DataFrame({\n","    'DateHour': dates,\n","    'Hour': dates.strftime('%H').astype('int'),\n","    'Year': dates.year,\n","    'MonthNumber': dates.month,\n","    'Month': dates.strftime('%B'),\n","    'MonthShort': dates.strftime('%b'),\n","    'Day': dates.strftime('%A').astype('str'),\n","    'DayShort': dates.strftime('%a'),\n","    'DayOfWeek': dates.weekday,\n","    'DayOfYear': dates.strftime('%j').astype('int'),\n","    'WeekOfYear': dates.strftime('%W').astype('int'),\n","    'YearQuarter': dates.year.astype('str') + \"Q\" + dates.quarter.astype('str'),\n","    'YearWeek': dates.strftime('%Y%W'),\n","    'EOM': pd.to_datetime(dates, format=\"%Y%m\") + MonthEnd(0),\n","    'EOQ': pd.to_datetime(dates, format=\"%Y%m\") + QuarterEnd(0)\n","})\n","\n","# Add date column\n","df['Date'] = pd.to_datetime(pd.to_datetime(df['DateHour']).dt.date)\n","\n","# Create index columns from today\n","today = datetime.today()\n","df['DaysFromToday'] = (df['Date']-today).dt.days\n","df['WeeksFromToday'] = ((df['Date']-today).dt.days/7).astype('int')\n","df['MonthsFromToday'] = ((df['Year']-today.year) * 12 + ( df['MonthNumber']-today.month))\n","df['QtrsFromToday'] = (df['MonthsFromToday'].astype('int')/3).astype('int')\n","df['YearsFromToday'] = df['Year']-today.year\n","\n","# Add Is Working Day\n","\n","df['WorkingDay'] = np.where((df['DayOfWeek'] != 5) & (df['DayOfWeek'] != 6), True, False)\n","\n","# Define function to add zero-width spaces for month and day names\n","def AddZWS(row, col, num):\n","    return (num-row[col]) * chr(8203)\n","\n","# Add ZWS columns for month and day\n","df['MonthZWS'] = df.apply(AddZWS, num=12, col='MonthNumber', axis=1) + df['Month']\n","df['DayZWS'] = df.apply(AddZWS, num=6, col='DayOfWeek', axis=1) + df['Day']\n","\n","# Convert pandas DF to spark DF and write to delta\n","# Remove spaces from column names (if any)\n","df.columns = df.columns.str.replace(' ', '')\n","\n","# Create Spark Datafram and Load to Delta Table\n","sparkDF = spark.createDataFrame(df)\n","sparkDF.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable(\"DateHour\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"956193d3-6680-4ed6-ae44-6c3174572e3f","statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","queued_time":"2024-05-14T12:03:23.7799163Z","session_start_time":null,"execution_start_time":"2024-05-14T12:04:54.1063705Z","execution_finish_time":"2024-05-14T12:04:59.1128232Z","parent_msg_id":"c96c85ac-2135-495a-8c33-eb12153e2d8c"},"text/plain":"StatementMeta(, 956193d3-6680-4ed6-ae44-6c3174572e3f, 8, Finished, Available)"},"metadata":{}}],"execution_count":6,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"220da884-dc67-4b48-b233-d76c7c117bc2"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"widgets":{},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{"lakehouse":{"default_lakehouse":"3314300a-4db0-46c0-8532-3ebeade52831","default_lakehouse_name":"MetricsAppData","default_lakehouse_workspace_id":"a85ef65e-ebea-40e1-86f0-e59b70e5254a"}}},"nbformat":4,"nbformat_minor":5}