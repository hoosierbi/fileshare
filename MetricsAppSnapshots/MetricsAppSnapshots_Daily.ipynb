{"cells":[{"cell_type":"code","source":["# Load need packages and provide Metrics App names\n","\n","!pip install semantic-link --q\n"," \n","import pandas as pd\n","import sempy.fabric as fabric\n","from datetime import datetime,date,timedelta\n","\n","MetricsWS = \"FabricMetricsApp_March2024\"\n","MetricsModel = \"Fabric Capacity Metrics\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"f9766359-5f4a-4e9b-b3d7-96e0854ab959","statement_id":3,"state":"finished","livy_statement_state":"available","queued_time":"2024-03-10T19:12:36.4391158Z","session_start_time":"2024-03-10T19:12:36.6417726Z","execution_start_time":"2024-03-10T19:12:46.6859354Z","execution_finish_time":"2024-03-10T19:13:16.3995017Z","parent_msg_id":"1b9391d4-c1e2-4271-820b-f8cfdcc1621c"},"text/plain":"StatementMeta(, f9766359-5f4a-4e9b-b3d7-96e0854ab959, 3, Finished, Available)"},"metadata":{}}],"execution_count":1,"metadata":{},"id":"007f42f2-304a-4079-a451-f5da494a743c"},{"cell_type":"code","source":["# Get Capacities, write to daily csv, overwrite Capacities Table\n","mssparkutils.fs.mkdirs(\"Files/Capacities/\")\n","df_capacities = fabric.evaluate_dax(workspace=MetricsWS, dataset=MetricsModel, dax_string=\"EVALUATE Capacities\")\n","df_capacities.columns = df_capacities.columns.str.replace(r'(.*\\[)|(\\].*)', '', regex=True)\n","df_capacities.columns = df_capacities.columns.str.replace(' ', '_')\n","#Need CU for each SKU to enable utilization calculation\n","skus = {'SKU': ['P1', 'P2', 'P3', 'P4', 'P5', 'F2', 'F4', 'F8', 'F16', 'F32', 'F64', 'F128', 'F256', 'F512', 'F1024', 'F2048', 'FT1', 'A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'DCT1', 'EM1', 'EM2', 'EM3'], 'CUperSecond': [64, 128, 256, 512, 1024, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 64, 8, 16, 32, 64, 128, 256, 512, 1024, 64, 8, 16, 32], 'CUperHour': [230400, 460800, 921600, 1843200, 3686400, 7200, 14400, 28800, 57600, 115200, 230400, 460800, 921600, 1843200, 3686400, 7372800, 230400, 28800, 57600, 115200, 230400, 460800, 921600, 1843200, 3686400, 230400, 28800, 57600, 115200]}\n","df_skus = pd.DataFrame(skus)\n","df_capacities = df_capacities.merge(df_skus, left_on=\"sku\", right_on=\"SKU\")\n","df_capacities = df_capacities.drop('SKU', axis=1)\n","filename = 'Capacities_' + (datetime.today()).strftime('%Y%m%d') + '.csv'\n","df_capacities.to_csv(\"/lakehouse/default/Files/Capacities/\" + filename)\n","spk_capacities = spark.createDataFrame(df_capacities)\n","spk_capacities.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable('Capacities')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"f9766359-5f4a-4e9b-b3d7-96e0854ab959","statement_id":4,"state":"finished","livy_statement_state":"available","queued_time":"2024-03-10T19:12:36.4397222Z","session_start_time":null,"execution_start_time":"2024-03-10T19:13:16.6832822Z","execution_finish_time":"2024-03-10T19:13:44.0948828Z","parent_msg_id":"09bebfe0-caa0-4e81-8710-39dfbbedca1d"},"text/plain":"StatementMeta(, f9766359-5f4a-4e9b-b3d7-96e0854ab959, 4, Finished, Available)"},"metadata":{}}],"execution_count":2,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"7459c7e9-55a0-4ee5-8a92-b75f78ac3dff"},{"cell_type":"code","source":["# Get Items, write to daily csv, overwrite Items Table\n","mssparkutils.fs.mkdirs(\"Files/Items/\")\n","df_items = fabric.evaluate_dax(workspace=MetricsWS, dataset=MetricsModel, dax_string=\"EVALUATE Items\")\n","df_items.columns = df_items.columns.str.replace(r'(.*\\[)|(\\].*)', '', regex=True)\n","df_items.columns = df_items.columns.str.replace(' ', '_')\n","df_items.drop_duplicates(subset=['ItemId'], keep='first', inplace=True)\n","filename = 'Items_' + (datetime.today()).strftime('%Y%m%d') + '.csv'\n","df_items.to_csv(\"/lakehouse/default/Files/Items/\" + filename)\n","spk_items = spark.createDataFrame(df_items)\n","spk_items.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable('Items')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"f9766359-5f4a-4e9b-b3d7-96e0854ab959","statement_id":5,"state":"finished","livy_statement_state":"available","queued_time":"2024-03-10T19:12:36.4437019Z","session_start_time":null,"execution_start_time":"2024-03-10T19:13:44.3715606Z","execution_finish_time":"2024-03-10T19:13:49.1058854Z","parent_msg_id":"569e6253-8010-40fd-bc25-de99849ba877"},"text/plain":"StatementMeta(, f9766359-5f4a-4e9b-b3d7-96e0854ab959, 5, Finished, Available)"},"metadata":{}}],"execution_count":3,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"0f167274-1fa7-4fe5-b084-062ecfe11594"},{"cell_type":"code","source":["# UsageData\n","# Get list of capacities\n","lst_capacities = df_capacities['capacityId'].tolist()\n","\n","# Create dir if doesn't exist\n","mssparkutils.fs.mkdirs(\"Files/UsageData/\")\n","\n","# Get max date from current delta table (to avoid loading duplicate days)\n","try:\n","    df_max = spark.sql(f'''\n","    SELECT MAX(Date) as MaxDate\n","    FROM usage;\n","    '''\n","    )\n","    maxdate = df_max.first()['MaxDate']\n","except:\n","    maxdate = datetime.today() + timedelta(days=-6)\n","    #Note - pulling too much data may result in 64 Mb error; reduce number of days for initial load\n","maxdateforDAX = maxdate.strftime('%Y,%m,%d')\n","\n","if maxdate.date() < (datetime.today() + timedelta(days=-1)).date():\n","\n","    # Get data for each capacity, write daily csv and append delta\n","    for capacity in lst_capacities:   \n","        querytext = '''\\\n","                    DEFINE MPARAMETER 'CapacityID' =  \"{capID}\"\n","                    var Yesterday = TODAY() - 1\n","\n","                    EVALUATE\n","                    CALCULATETABLE(MetricsByItemandOperationandHour, MetricsByItemandOperationandHour[Date] <= Yesterday && MetricsByItemandOperationandHour[Date] > DATE({MD}))\n","                    '''.format(capID=capacity, MD=maxdateforDAX)\n","        df_hourly = fabric.evaluate_dax(workspace=MetricsWS, dataset=MetricsModel, dax_string=querytext)\n","        if len(df_hourly) >= 1:\n","            df_hourly.columns = df_hourly.columns.str.replace(r'(.*\\[)|(\\].*)', '', regex=True)\n","            df_hourly.columns = df_hourly.columns.str.replace(' ', '_')\n","            df_hourly.rename(columns={'Throttling_(min)': 'ThrottlingMin'}, inplace=True)\n","            filename = capacity + '_Usage_' + (datetime.today()).strftime('%Y%m%d') + '.csv'\n","            df_hourly.to_csv(\"/lakehouse/default/Files/UsageData/\" + filename)\n","            spk_usage = spark.createDataFrame(df_hourly)\n","            spk_usage.write.mode(\"append\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable('Usage')\n","else:\n","    print(\"Data already loaded\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"f9766359-5f4a-4e9b-b3d7-96e0854ab959","statement_id":6,"state":"finished","livy_statement_state":"available","queued_time":"2024-03-10T19:12:36.4442797Z","session_start_time":null,"execution_start_time":"2024-03-10T19:13:49.4046221Z","execution_finish_time":"2024-03-10T19:14:10.1318672Z","parent_msg_id":"ed22eeb6-fedc-481e-849c-8715b8261532"},"text/plain":"StatementMeta(, f9766359-5f4a-4e9b-b3d7-96e0854ab959, 6, Finished, Available)"},"metadata":{}}],"execution_count":4,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"f147c831-edcc-46b1-8fec-fc72f204590c"},{"cell_type":"code","source":["# ThrottlingData\n","# Get list of capacities\n","lst_capacities = df_capacities['capacityId'].tolist()\n","\n","# Create dir if doesn't exist\n","mssparkutils.fs.mkdirs(\"Files/ThrottlingData/\")\n","\n","# Get max date from current delta table (to avoid loading duplicate days)\n","try:\n","    df_max = spark.sql(f'''\n","    SELECT MAX(Date) as MaxDate\n","    FROM throttling;\n","    '''\n","    )\n","    maxdate = df_max.first()['MaxDate']\n","except:\n","    maxdate = datetime.today() + timedelta(days=-6)\n","maxdateforDAX = maxdate.strftime('%Y,%m,%d')\n","\n","if maxdate.date() < (datetime.today() + timedelta(days=-1)).date():\n","\n","    # Get data for each capacity, write daily csv and append delta\n","    for capacity in lst_capacities:   \n","        querytext = '''\\\n","                    DEFINE\n","                    MPARAMETER 'CapacityID' = \"{capID}\"\n","                    VAR yesterday =\n","                        FILTER(ALL('Dates'[Date] ), 'Dates'[Date] < TODAY() && 'Dates'[Date] > DATE({MD}) )\n","\n","                    EVALUATE\n","                    SUMMARIZECOLUMNS(\n","                        'Dates'[Date],\n","                        'TimePoints'[Start of Hour],\n","                        yesterday,\n","                        \"IntDelay\", ROUND( 'All Measures'[Dynamic InteractiveDelay %] * 100, 2 ),\n","                        \"IntReject\", ROUND( 'All Measures'[Dynamic InteractiveRejection %] * 100, 2 ),\n","                        \"BackReject\", ROUND( 'All Measures'[Dynamic BackgroundRejection %] * 100, 2 )\n","                    )\n","                    '''.format(capID=capacity, MD=maxdateforDAX)\n","        df_throttling = fabric.evaluate_dax(workspace=MetricsWS, dataset=MetricsModel, dax_string=querytext)\n","        if len(df_throttling) >= 1:\n","            df_throttling.columns = df_throttling.columns.str.replace(r'(.*\\[)|(\\].*)', '', regex=True)\n","            df_throttling.columns = df_throttling.columns.str.replace(' ', '_')\n","            df_throttling['capacityId'] = capacity\n","            filename = capacity + '_throttling_' + (datetime.today()).strftime('%Y%m%d') + '.csv'\n","            df_throttling.to_csv(\"/lakehouse/default/Files/ThrottlingData/\" + filename)\n","            spk_throttle = spark.createDataFrame(df_throttling)\n","            spk_throttle.write.mode(\"append\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable('Throttling')\n","\n","else:\n","    print(\"Data already loaded\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"f9766359-5f4a-4e9b-b3d7-96e0854ab959","statement_id":7,"state":"finished","livy_statement_state":"available","queued_time":"2024-03-10T19:12:36.4447754Z","session_start_time":null,"execution_start_time":"2024-03-10T19:14:10.4106006Z","execution_finish_time":"2024-03-10T19:14:24.5429096Z","parent_msg_id":"9f736039-f5db-4169-a1fa-af3b80c59204"},"text/plain":"StatementMeta(, f9766359-5f4a-4e9b-b3d7-96e0854ab959, 7, Finished, Available)"},"metadata":{}}],"execution_count":5,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"3f452c05-2fca-4fdf-8115-6076f9a0938d"},{"cell_type":"code","source":["# DateHour Table\n","import pandas as pd\n","from datetime import datetime\n","import numpy as np\n","import math\n","from pandas.tseries.offsets import MonthEnd, QuarterEnd\n","\n","# Create a list of dates from 2020 to 2024\n","start_date = datetime(2024, 2, 24)\n","end_date = pd.to_datetime(\"today\") #datetime(2024, 12, 31)\n","dates = pd.date_range(start=start_date, end=end_date, freq = '1H')\n","\n","# Create a dataframe with the desired columns\n","df = pd.DataFrame({\n","    'DateHour': dates,\n","    'Hour': dates.strftime('%H').astype('int'),\n","    'Year': dates.year,\n","    'MonthNumber': dates.month,\n","    'Month': dates.strftime('%B'),\n","    'MonthShort': dates.strftime('%b'),\n","    'Day': dates.strftime('%A').astype('str'),\n","    'DayShort': dates.strftime('%a'),\n","    'DayOfWeek': dates.weekday,\n","    'DayOfYear': dates.strftime('%j').astype('int'),\n","    'WeekOfYear': dates.strftime('%W').astype('int'),\n","    'YearQuarter': dates.year.astype('str') + \"Q\" + dates.quarter.astype('str'),\n","    'YearWeek': dates.strftime('%Y%W'),\n","    'EOM': pd.to_datetime(dates, format=\"%Y%m\") + MonthEnd(0),\n","    'EOQ': pd.to_datetime(dates, format=\"%Y%m\") + QuarterEnd(0)\n","})\n","\n","# Add date column\n","df['Date'] = pd.to_datetime(pd.to_datetime(df['DateHour']).dt.date)\n","\n","# Create index columns from today\n","today = datetime.today()\n","df['DaysFromToday'] = (df['Date']-today).dt.days\n","df['WeeksFromToday'] = ((df['Date']-today).dt.days/7).astype('int')\n","df['MonthsFromToday'] = ((df['Year']-today.year) * 12 + ( df['MonthNumber']-today.month))\n","df['QtrsFromToday'] = (df['MonthsFromToday'].astype('int')/3).astype('int')\n","df['YearsFromToday'] = df['Year']-today.year\n","\n","# Add Is Working Day\n","\n","df['WorkingDay'] = np.where((df['DayOfWeek'] != 5) & (df['DayOfWeek'] != 6), True, False)\n","\n","# Define function to add zero-width spaces for month and day names\n","def AddZWS(row, col, num):\n","    return (num-row[col]) * chr(8203)\n","\n","# Add ZWS columns for month and day\n","df['MonthZWS'] = df.apply(AddZWS, num=12, col='MonthNumber', axis=1) + df['Month']\n","df['DayZWS'] = df.apply(AddZWS, num=6, col='DayOfWeek', axis=1) + df['Day']\n","\n","# Convert pandas DF to spark DF and write to delta\n","# Remove spaces from column names (if any)\n","df.columns = df.columns.str.replace(' ', '')\n","\n","# Create Spark Datafram and Load to Delta Table\n","sparkDF = spark.createDataFrame(df)\n","sparkDF.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable(\"DateHour\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"f9766359-5f4a-4e9b-b3d7-96e0854ab959","statement_id":8,"state":"finished","livy_statement_state":"available","queued_time":"2024-03-10T19:12:36.4453156Z","session_start_time":null,"execution_start_time":"2024-03-10T19:14:24.8270598Z","execution_finish_time":"2024-03-10T19:14:28.2507237Z","parent_msg_id":"07f0b864-2310-43db-9e78-edb16873085d"},"text/plain":"StatementMeta(, f9766359-5f4a-4e9b-b3d7-96e0854ab959, 8, Finished, Available)"},"metadata":{}}],"execution_count":6,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"220da884-dc67-4b48-b233-d76c7c117bc2"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"microsoft":{"language":"python"},"widgets":{},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"trident":{"lakehouse":{"default_lakehouse":"063a582e-5bfc-4de8-918b-3a31084c3447","default_lakehouse_name":"MetricsSnapshots","default_lakehouse_workspace_id":"bb3993d7-a4f7-4502-b8be-e32eb595a9f0"}}},"nbformat":4,"nbformat_minor":5}