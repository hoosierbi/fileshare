{"cells":[{"cell_type":"code","execution_count":null,"id":"007f42f2-304a-4079-a451-f5da494a743c","metadata":{},"outputs":[],"source":["# Load need packages and provide Metrics App names\n","\n","!pip install semantic-link --q\n"," \n","import pandas as pd\n","import sempy.fabric as fabric\n","from datetime import datetime,date,timedelta\n","\n","#Update with names from your install of the Fabric Metrics Apps\n","#Note - This code works with V1.5 of Metrics App (March 2024). Breaking changes possible in later updates.\n","MetricsWS = \"FabricMetricsApp_March2024\"\n","MetricsModel = \"Fabric Capacity Metrics\""]},{"cell_type":"code","execution_count":null,"id":"7459c7e9-55a0-4ee5-8a92-b75f78ac3dff","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Get Capacities, write to daily csv, overwrite Capacities Table\n","mssparkutils.fs.mkdirs(\"Files/Capacities/\")\n","df_capacities = fabric.evaluate_dax(workspace=MetricsWS, dataset=MetricsModel, dax_string=\"EVALUATE Capacities\")\n","df_capacities.columns = df_capacities.columns.str.replace(r'(.*\\[)|(\\].*)', '', regex=True)\n","df_capacities.columns = df_capacities.columns.str.replace(' ', '_')\n","#Need CU for each SKU to enable utilization calculation\n","skus = {'SKU': ['P1', 'P2', 'P3', 'P4', 'P5', 'F2', 'F4', 'F8', 'F16', 'F32', 'F64', 'F128', 'F256', 'F512', 'F1024', 'F2048', 'FT1', 'A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'DCT1', 'EM1', 'EM2', 'EM3'], 'CUperSecond': [64, 128, 256, 512, 1024, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 64, 8, 16, 32, 64, 128, 256, 512, 1024, 64, 8, 16, 32], 'CUperHour': [230400, 460800, 921600, 1843200, 3686400, 7200, 14400, 28800, 57600, 115200, 230400, 460800, 921600, 1843200, 3686400, 7372800, 230400, 28800, 57600, 115200, 230400, 460800, 921600, 1843200, 3686400, 230400, 28800, 57600, 115200]}\n","df_skus = pd.DataFrame(skus)\n","df_capacities = df_capacities.merge(df_skus, left_on=\"sku\", right_on=\"SKU\")\n","df_capacities = df_capacities.drop('SKU', axis=1)\n","filename = 'Capacities_' + (datetime.today()).strftime('%Y%m%d') + '.csv'\n","df_capacities.to_csv(\"/lakehouse/default/Files/Capacities/\" + filename)\n","spk_capacities = spark.createDataFrame(df_capacities)\n","spk_capacities.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable('Capacities')"]},{"cell_type":"code","execution_count":null,"id":"0f167274-1fa7-4fe5-b084-062ecfe11594","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Get Items, write to daily csv, overwrite Items Table\n","mssparkutils.fs.mkdirs(\"Files/Items/\")\n","df_items = fabric.evaluate_dax(workspace=MetricsWS, dataset=MetricsModel, dax_string=\"EVALUATE Items\")\n","df_items.columns = df_items.columns.str.replace(r'(.*\\[)|(\\].*)', '', regex=True)\n","df_items.columns = df_items.columns.str.replace(' ', '_')\n","df_items.drop_duplicates(subset=['ItemId'], keep='first', inplace=True)\n","filename = 'Items_' + (datetime.today()).strftime('%Y%m%d') + '.csv'\n","df_items.to_csv(\"/lakehouse/default/Files/Items/\" + filename)\n","spk_items = spark.createDataFrame(df_items)\n","spk_items.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable('Items')"]},{"cell_type":"code","execution_count":null,"id":"f147c831-edcc-46b1-8fec-fc72f204590c","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# UsageData\n","# Get list of capacities\n","lst_capacities = df_capacities['capacityId'].tolist()\n","\n","# Create dir if doesn't exist\n","mssparkutils.fs.mkdirs(\"Files/UsageData/\")\n","\n","# Get max date from current delta table (to avoid loading duplicate days)\n","try:\n","    df_max = spark.sql(f'''\n","    SELECT MAX(Date) as MaxDate\n","    FROM usage;\n","    '''\n","    )\n","    maxdate = df_max.first()['MaxDate']\n","except:\n","    maxdate = datetime.today() + timedelta(days=-6)\n","    #Note - pulling too much data may result in 64 Mb error; reduce number of days for initial load\n","maxdateforDAX = maxdate.strftime('%Y,%m,%d')\n","\n","if maxdate.date() < (datetime.today() + timedelta(days=-1)).date():\n","\n","    # Get data for each capacity, write daily csv and append delta\n","    for capacity in lst_capacities:   \n","        querytext = '''\\\n","                    DEFINE MPARAMETER 'CapacityID' =  \"{capID}\"\n","                    var Yesterday = TODAY() - 1\n","\n","                    EVALUATE\n","                    CALCULATETABLE(MetricsByItemandOperationandHour, MetricsByItemandOperationandHour[Date] <= Yesterday && MetricsByItemandOperationandHour[Date] > DATE({MD}))\n","                    '''.format(capID=capacity, MD=maxdateforDAX)\n","        df_hourly = fabric.evaluate_dax(workspace=MetricsWS, dataset=MetricsModel, dax_string=querytext)\n","        if len(df_hourly) >= 1:\n","            df_hourly.columns = df_hourly.columns.str.replace(r'(.*\\[)|(\\].*)', '', regex=True)\n","            df_hourly.columns = df_hourly.columns.str.replace(' ', '_')\n","            df_hourly.rename(columns={'Throttling_(min)': 'ThrottlingMin'}, inplace=True)\n","            filename = capacity + '_Usage_' + (datetime.today()).strftime('%Y%m%d') + '.csv'\n","            df_hourly.to_csv(\"/lakehouse/default/Files/UsageData/\" + filename)\n","            spk_usage = spark.createDataFrame(df_hourly)\n","            spk_usage.write.mode(\"append\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable('Usage')\n","else:\n","    print(\"Data already loaded\")"]},{"cell_type":"code","execution_count":null,"id":"3f452c05-2fca-4fdf-8115-6076f9a0938d","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# ThrottlingData\n","# Get list of capacities\n","lst_capacities = df_capacities['capacityId'].tolist()\n","\n","# Create dir if doesn't exist\n","mssparkutils.fs.mkdirs(\"Files/ThrottlingData/\")\n","\n","# Get max date from current delta table (to avoid loading duplicate days)\n","try:\n","    df_max = spark.sql(f'''\n","    SELECT MAX(Date) as MaxDate\n","    FROM throttling;\n","    '''\n","    )\n","    maxdate = df_max.first()['MaxDate']\n","except:\n","    maxdate = datetime.today() + timedelta(days=-6)\n","maxdateforDAX = maxdate.strftime('%Y,%m,%d')\n","\n","if maxdate.date() < (datetime.today() + timedelta(days=-1)).date():\n","\n","    # Get data for each capacity, write daily csv and append delta\n","    for capacity in lst_capacities:   \n","        querytext = '''\\\n","                    DEFINE\n","                    MPARAMETER 'CapacityID' = \"{capID}\"\n","                    VAR yesterday =\n","                        FILTER(ALL('Dates'[Date] ), 'Dates'[Date] < TODAY() && 'Dates'[Date] > DATE({MD}) )\n","\n","                    EVALUATE\n","                    SUMMARIZECOLUMNS(\n","                        'Dates'[Date],\n","                        'TimePoints'[Start of Hour],\n","                        yesterday,\n","                        \"IntDelay\", ROUND( 'All Measures'[Dynamic InteractiveDelay %] * 100, 2 ),\n","                        \"IntReject\", ROUND( 'All Measures'[Dynamic InteractiveRejection %] * 100, 2 ),\n","                        \"BackReject\", ROUND( 'All Measures'[Dynamic BackgroundRejection %] * 100, 2 )\n","                    )\n","                    '''.format(capID=capacity, MD=maxdateforDAX)\n","        df_throttling = fabric.evaluate_dax(workspace=MetricsWS, dataset=MetricsModel, dax_string=querytext)\n","        if len(df_throttling) >= 1:\n","            df_throttling.columns = df_throttling.columns.str.replace(r'(.*\\[)|(\\].*)', '', regex=True)\n","            df_throttling.columns = df_throttling.columns.str.replace(' ', '_')\n","            df_throttling['capacityId'] = capacity\n","            filename = capacity + '_throttling_' + (datetime.today()).strftime('%Y%m%d') + '.csv'\n","            df_throttling.to_csv(\"/lakehouse/default/Files/ThrottlingData/\" + filename)\n","            spk_throttle = spark.createDataFrame(df_throttling)\n","            spk_throttle.write.mode(\"append\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable('Throttling')\n","\n","else:\n","    print(\"Data already loaded\")"]},{"cell_type":"code","execution_count":null,"id":"220da884-dc67-4b48-b233-d76c7c117bc2","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# DateHour Table\n","import pandas as pd\n","from datetime import datetime\n","import numpy as np\n","import math\n","from pandas.tseries.offsets import MonthEnd, QuarterEnd\n","\n","# Create a list of dates from 2020 to 2024\n","start_date = datetime(2024, 2, 24)\n","end_date = pd.to_datetime(\"today\") #datetime(2024, 12, 31)\n","dates = pd.date_range(start=start_date, end=end_date, freq = '1H')\n","\n","# Create a dataframe with the desired columns\n","df = pd.DataFrame({\n","    'DateHour': dates,\n","    'Hour': dates.strftime('%H').astype('int'),\n","    'Year': dates.year,\n","    'MonthNumber': dates.month,\n","    'Month': dates.strftime('%B'),\n","    'MonthShort': dates.strftime('%b'),\n","    'Day': dates.strftime('%A').astype('str'),\n","    'DayShort': dates.strftime('%a'),\n","    'DayOfWeek': dates.weekday,\n","    'DayOfYear': dates.strftime('%j').astype('int'),\n","    'WeekOfYear': dates.strftime('%W').astype('int'),\n","    'YearQuarter': dates.year.astype('str') + \"Q\" + dates.quarter.astype('str'),\n","    'YearWeek': dates.strftime('%Y%W'),\n","    'EOM': pd.to_datetime(dates, format=\"%Y%m\") + MonthEnd(0),\n","    'EOQ': pd.to_datetime(dates, format=\"%Y%m\") + QuarterEnd(0)\n","})\n","\n","# Add date column\n","df['Date'] = pd.to_datetime(pd.to_datetime(df['DateHour']).dt.date)\n","\n","# Create index columns from today\n","today = datetime.today()\n","df['DaysFromToday'] = (df['Date']-today).dt.days\n","df['WeeksFromToday'] = ((df['Date']-today).dt.days/7).astype('int')\n","df['MonthsFromToday'] = ((df['Year']-today.year) * 12 + ( df['MonthNumber']-today.month))\n","df['QtrsFromToday'] = (df['MonthsFromToday'].astype('int')/3).astype('int')\n","df['YearsFromToday'] = df['Year']-today.year\n","\n","# Add Is Working Day\n","\n","df['WorkingDay'] = np.where((df['DayOfWeek'] != 5) & (df['DayOfWeek'] != 6), True, False)\n","\n","# Define function to add zero-width spaces for month and day names\n","def AddZWS(row, col, num):\n","    return (num-row[col]) * chr(8203)\n","\n","# Add ZWS columns for month and day\n","df['MonthZWS'] = df.apply(AddZWS, num=12, col='MonthNumber', axis=1) + df['Month']\n","df['DayZWS'] = df.apply(AddZWS, num=6, col='DayOfWeek', axis=1) + df['Day']\n","\n","# Convert pandas DF to spark DF and write to delta\n","# Remove spaces from column names (if any)\n","df.columns = df.columns.str.replace(' ', '')\n","\n","# Create Spark Datafram and Load to Delta Table\n","sparkDF = spark.createDataFrame(df)\n","sparkDF.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable(\"DateHour\")"]}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python"},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"trident":{"lakehouse":{"default_lakehouse":"063a582e-5bfc-4de8-918b-3a31084c3447","default_lakehouse_name":"MetricsSnapshots","default_lakehouse_workspace_id":"bb3993d7-a4f7-4502-b8be-e32eb595a9f0"}},"widgets":{}},"nbformat":4,"nbformat_minor":5}
