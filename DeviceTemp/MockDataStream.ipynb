{"cells":[{"cell_type":"code","source":["%pip install azure-eventhub"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":true},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a6cfeb12-08de-4041-8dea-5af81b6444a3"},{"cell_type":"code","source":["# This cell generates random temp and humidity data for a defined number of devices\n","import time\n","import os\n","import datetime\n","import random\n","import json\n","from azure.eventhub import EventHubProducerClient,EventData\n","\n","# Creates an array of devices to send data for\n","devices = []\n","for x in range(1, 11):\n","    devices.append(int(x))\n","print(devices)\n","\n","#Get from the Custom App source of your Fabric EventStream (the Event Hub Name and the Connection String-Primary Key)\n","EventHubName = 'es_9f9c...'\n","EventHubEndpoint = 'Endpoint=sb://esehblfl...'\n","\n","# Create a producer client to produce and publish events to the event hub.\n","producer = EventHubProducerClient.from_connection_string(conn_str=EventHubEndpoint, eventhub_name=EventHubName)\n","\n","# Update the range to specify the number of iterations, the data provided in the reading JSON packet, and/or the seconds between each batch of data\n","for y in range(0,6):\n","    event_data_batch = producer.create_batch() # Create a batch. You will add events to the batch later. \n","    for dev in devices:\n","        # Create a dummy reading.\n","        reading = {\n","            'id': dev, \n","            'timestamp': str(datetime.datetime.utcnow()),\n","            'temperature': random.randint(80, 92), \n","            'humidity': random.randint(80, 100)\n","        }\n","        s = json.dumps(reading) # Convert the reading into a JSON string.\n","        print(s)\n","        event_data_batch.add(EventData(s)) # Add event data to the batch.\n","    producer.send_batch(event_data_batch) # Send the batch of events to the event hub.\n","    time.sleep(15)\n","producer.close()"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"jupyter":{"outputs_hidden":true},"editable":true,"run_control":{"frozen":false}},"id":"3d415762-794c-49d0-807c-37d5ca9fc748"},{"cell_type":"code","source":["# This cell sends a filtered subset of rows from a dataframe to an EventStream every x seconds. Use the provided csv url, or create your own dataframe and update the code\n","import pandas as pd\n","import time\n","import os\n","import datetime\n","import json\n","from azure.eventhub import EventHubProducerClient,EventData\n","\n","#Source data - use this url or create your own mock data/dataframe\n","url = 'https://raw.githubusercontent.com/hoosierbi/fileshare/main/DeviceTemp/DeviceTemps.csv'\n","\n","#Get from the Custom App source of your Fabric EventStream (the Event Hub Name and the Connection String-Primary Key)\n","EventHubName = 'es_847505...'\n","EventHubEndpoint = 'Endpoint=sb://esehblrhf...'\n","\n","#Read in the csv, unpivot the data, and filter to a few of the devices\n","data_raw = pd.read_csv(url)\n","data_unpivot = pd.melt(data_raw, id_vars='Batch')\n","data = data_unpivot.rename(columns={'variable': 'Device', 'value': 'Temp'})\n","data = data[data['Device'].isin(['D003', 'D004', 'D007', 'D011', 'D014', 'D016', 'D017'])]\n","\n","# Create a producer client to produce and publish events to the event hub.\n","producer = EventHubProducerClient.from_connection_string(conn_str=EventHubEndpoint, eventhub_name=EventHubName)\n","\n","# Data has 500 batches of data in it. Iterate through all of them. Update the time.sleep for seconds between batches.\n","for y in range(1,501):\n","    event_data_batch = producer.create_batch() # Create a batch. You will add events to the batch later. \n","    devices = data[data['Batch'] == y]\n","\n","    for row in devices.itertuples(index=False):\n","        reading = {\n","            'id': row.Device, \n","            'timestamp': str(datetime.datetime.utcnow()),\n","            'temperature': row.Temp\n","        }\n","        s = json.dumps(reading) # Convert the reading into a JSON string.\n","        print(s)\n","        event_data_batch.add(EventData(s)) # Add event data to the batch.\n","    producer.send_batch(event_data_batch) # Send the batch of events to the event hub.\n","    time.sleep(5) #wait time in seconds\n","producer.close()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9c2d8b7c-a5cd-4d70-bc69-a4e485d6219c"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"environment":{}}},"nbformat":4,"nbformat_minor":5}
