{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# <span style=\"color: blue;\">**DF Market** - _A Fabric Playtaset_ </span>\n",
                "\n",
                "## About the solution\n",
                "- This notebook will deploy multiple related Fabric items that can be used to generate sample data of variable size for a fictional grocery store chain called \"DF Market\" (DF for Data Factory)\n",
                "- This solution intentionally uses multiple Fabric items so that it may be useful both to demo/test Fabric workloads and/or to demo/test data analysis/visualization scenarios\n",
                "- The resulting model has 4 dimension tables (Stores, Products, Date, and Time) and one fact table called Sales\n",
                "- Seed values are used in the solution so that the same data will be generated each time for the same input values.\n",
                "- The data has some useful patterns/fields in it:\n",
                "    - one store in each city but different sales volume that change at different rates over time\n",
                "    - sales at each hour differ by day of the week\n",
                "    - stores have open/close dates and products have launch/discontinue dates\n",
                "    - cities have lat/long for mapping\n",
                "    - sales data has 1+ products per transaction for distinct count scenarios\n",
                "    - stores table has store manager email for RLS and/or report bursting scenarios\n",
                "    - the Stores, Products, and Date table are created last and are filtered to only include rows that exist in the Sales table\n",
                "\n",
                "\n",
                "## How to Deploy\n",
                "- Hit \"Run All\" above or run each cell ***in order*** \n",
                "- Once all items are deployed\n",
                "    - Go back to the Workspace, to confirm all items are created. Note: this solution creates two Direct Lake semantic models and reports. One uses the Lakehouse SQL Endpoint (\"_SE\") and the other is Direct Lake over Onelake (\"_OL\"). If one or both reports are not visible, refresh the browser and they should show up.\n",
                "    - Open each of the two Dataflows (append sales and replace dims) and click on \"Manage Connections\". There will likely be no connection. Choose \"Create New Connection\" and choose Lakehouse and accept any defaults. ***Save the Dataflow (not \"Save And Run\") and repeat the same steps for the other dataflow (using the same new connection).*** This updates the connection and since there is a change, the save action publishes the dataflow. If the connection was already established (e.g., you ran this solution previously), make a small change (e.g., add a space in the formula bar of the last step for one of the queries) and save it to force a publish.\n",
                "    - Open the Generate_Data pipeline and review the pipeline Variables (starting month and number of months), also review the Dataflow parameters in the ForEach with the Append_Sales_Monthly dataflow activity. To see those parameters, click on the dataflow activity inside the ForEach, click on \"Settings\", and expand \"Dataflow Parameters\". Only change these parameter values - AvgTransactionsPerDay, AvgProductsPerTransaction, AvgQtyPerProduct, MaxNumberOfProducts, and MaxNumberOfStores. The number of months, AvgTransactionsPerDay, AvgProductsPerTransaction and MaxNumberOfStores will have a direct impact on the number of Sales rows generated. The others will not and may cause refresh failures if changed.\n",
                "    - Save and Run the pipeline. With the default settings, about 170 million Sales table rows will be generated in about 25 min. **Note this gets very big fast, so don't max out the values!** Do the math. For example, if you double the number of stores and double the transactions per day per store, you should get about 4X more. Some examples are listed below in the table.\n",
                "    \n",
                "\n",
                "| Start Month | Number Of Months | AvgTransactionPerDay | AvgProductsPerTransaction | MaxNumberOfStores | Row Count | Duration (min) |\n",
                "|:-----------:|:----------------:|:--------------------:|:-------------------------:|:-----------------:|----------:|:--------------:|\n",
                "|2025-01|3|25|4|10|395,130|5|\n",
                "|2025-01|6|50|3|20|5,741,781|5|\n",
                "|2025-01|6|10|3|1000|27,177,744|8|\n",
                "|2024-01|18|250|4|60|171,232,712|21|\n",
                "|2021-01|54|425|4|100|1,173,902,423|52|\n",
                "\n",
                "**<div align=\"center\"> The default values are the 4th row (170M)</div>**\n",
                "\n",
                "\n",
                "- Note: the ForEach to append monthly data in the pipeline has a default batch count of 12. There is a random wait time in that ForEach to reduce storage access conflicts. If you see \"fail\"s while it's running, there is retry logic set on the ForEach so it should still work. If not, rerun with a lower batch count or increase the random wait time upper limit.\n",
                "- When the pipeline finishes successfully, check the report to see how many rows were created. If any issues, do these additional steps:\n",
                "    - Open the Lakehouse SQL Endpoint and hit the Metadata Sync button in the ribbon.\n",
                "    - Refresh the semantic model in the Workspace UI.\n",
                "\n",
                "\n",
                "## Future Plans\n",
                "- This is V1 and more is planned (e.g., incremental refresh, copy jobs to load data to WH and Eventhouse, materialized lake view aggregation tables w/ updated semantic model)\n",
                "- Your feedback and ideas are welcomed. Please also share any additional items you create that may also be useful to others.\n"
            ],
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                },
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "b85f5796-2251-4ba1-aca8-a37f559ca989"
        },
        {
            "cell_type": "code",
            "source": [
                "import requests\n",
                "import pandas as pd\n",
                "import sempy.fabric as fabric\n",
                "from sempy.fabric.exceptions import FabricHTTPException, WorkspaceNotFoundException\n",
                "import requests\n",
                "import base64\n",
                "import json\n",
                "import time\n",
                "try:\n",
                "    import sempy_labs as labs\n",
                "    print('labs already installed')\n",
                "except:\n",
                "    print('installing labs')\n",
                "    %pip install semantic-link-labs\n",
                "    import sempy_labs as labs"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "jupyter": {
                    "outputs_hidden": true
                }
            },
            "id": "3b6fe170-cde5-4e91-a993-6a70d8ebc046"
        },
        {
            "cell_type": "code",
            "source": [
                "newids = {\n",
                "    'Workspace_DFMarket_GUID': '',\n",
                "    'Lakehouse_DF_Market_LH_GUID': '',\n",
                "    'Report_DF_Market_Report_SE_GUID': '',\n",
                "    'Report_DF_Market_Report_OL_GUID': '', \n",
                "    'SemanticModel_DF_Market_SM_GUID': '',\n",
                "    'SemanticModel_DF_Market_OL_GUID': '',\n",
                "    'Notebook_Drop_Create_Sales_Table_GUID': '', \n",
                "    'Dataflow_Append_Sales_Table_GUID': '', \n",
                "    'Dataflow_Replace_DIM_Tables_GUID': '', \n",
                "    'DataPipeline_Generate_Data_GUID': '' ,\n",
                "    'Lakehouse_DF_Market_LH_SQLEndpoint': '',\n",
                "    'Lakehouse_DF_Market_LH_DatabaseId': '',\n",
                "    'OneLakeRegionPrefix': ''\n",
                "}\n",
                "thisworkspaceid = spark.conf.get(\"trident.workspace.id\")\n",
                "newids['Workspace_DFMarket_GUID'] = thisworkspaceid\n",
                "\n",
                "newids"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "a16c479f-0618-4bfb-bf1c-80425d392be4"
        },
        {
            "cell_type": "code",
            "source": [
                "# Create DF Market Lakehouse\n",
                "access_token = notebookutils.credentials.getToken(\"pbi\")\n",
                "headers = {\"Authorization\": f\"Bearer {access_token}\",\n",
                "            \"Content-Type\": \"application/json\"}\n",
                "url = f\"https://api.fabric.microsoft.com/v1/workspaces/{thisworkspaceid}/lakehouses\"\n",
                "body = {\n",
                "  \"displayName\": \"DF_Market_LH\",\n",
                "  \"description\": \"DF Market Lakehouse\"\n",
                "}\n",
                "response = requests.post(url, headers=headers, json=body)\n",
                "jsonresponse = response.json()\n",
                "print(jsonresponse)\n",
                "lakehouseid = jsonresponse['id']\n",
                "\n",
                "# Add new LH id to newids\n",
                "newids['Lakehouse_DF_Market_LH_GUID'] = lakehouseid\n",
                "newids"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "337d9078-6da7-42ce-8a6c-74ba4e7176d1"
        },
        {
            "cell_type": "code",
            "source": [
                "# Get Lakehouse SQL Endpoint\n",
                "time.sleep(30) #gives time to create lakehouse and sql endpoint if \"Run All\" is used. Comment it out if you run each cell manually and repeat it until you see sqlendpoint and databaseid values in the output.\n",
                "access_token = notebookutils.credentials.getToken(\"pbi\")\n",
                "headers = {\"Authorization\": f\"Bearer {access_token}\",\n",
                "            \"Content-Type\": \"application/json\"}\n",
                "url = f\"https://api.fabric.microsoft.com/v1/workspaces/{thisworkspaceid}/lakehouses/{lakehouseid}\"\n",
                "\n",
                "response = requests.get(url, headers=headers)\n",
                "jsonresponse = response.json()\n",
                "# print(jsonresponse)\n",
                "\n",
                "# Add new LH info to newids\n",
                "newids['Lakehouse_DF_Market_LH_SQLEndpoint'] = jsonresponse['properties']['sqlEndpointProperties']['connectionString']\n",
                "newids['Lakehouse_DF_Market_LH_DatabaseId'] = jsonresponse['properties']['sqlEndpointProperties']['id']\n",
                "newids"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "28d8aeda-311a-4581-8af1-1c76fb22cdec"
        },
        {
            "cell_type": "markdown",
            "source": [
                "### *Note - Sometimes the SQL Endpoint generation takes longer than the time.sleep wait time. Make sure the sql endpoint and datamart id values are populated in the output of the cell above before continuing. Wait 10-20s and rerun the cell until you see values populated.*"
            ],
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                },
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "e17ab4cd-e063-4263-ac2b-f6be6665e796"
        },
        {
            "cell_type": "code",
            "source": [
                "# Get Lakehouse Region Prefix\n",
                "onelakeblob = fabric.FabricRestClient().get(f\"/v1/workspaces/{thisworkspaceid}\").json()['oneLakeEndpoints']['blobEndpoint']\n",
                "regionprefix = onelakeblob.split(\"//\")[1].split(\"onelake\")[0]\n",
                "newids['OneLakeRegionPrefix'] = regionprefix\n",
                "regionprefix"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "086459f4-08ce-47ee-b1d5-2d858c628fe3"
        },
        {
            "cell_type": "code",
            "source": [
                "# for troubleshooting\n",
                "# lakehouseid = 'f80140a9-881b-4437-86fc-39c8baf87aef'\n",
                "# thisworkspaceid = '17302819-7995-4a37-9d9f-86e2c5d2b2c3'"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "editable": false,
                "run_control": {
                    "frozen": true
                }
            },
            "id": "217de458-0c7e-4fa3-9017-22920833f683"
        },
        {
            "cell_type": "code",
            "source": [
                "url = \"https://raw.githubusercontent.com/hoosierbi/fileshare/refs/heads/main/DFMarketFiles/V1/DFMarket_V1.json\"\n",
                "deployjson = requests.get(url).text\n",
                "deploy_df = pd.read_json(deployjson)\n",
                "deploy_df['ReplaceString'] = deploy_df['type'] + '_' + deploy_df['displayName'].replace(' ', '_') + '_GUID'\n",
                "deploy_df\n"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "2c0e87b3-f2f0-48e0-a48b-976f9bcbfaf6"
        },
        {
            "cell_type": "code",
            "source": [
                "# Define Functions\n",
                "def tobase64(textstring):\n",
                "    textstring_bytes = textstring.encode(\"ascii\")\n",
                "    ascii_bytes = base64.b64encode(textstring_bytes)\n",
                "    base64_output = ascii_bytes.decode(\"ascii\")\n",
                "    return base64_output\n",
                "\n",
                "def convertpayloadstobase64(definitionjson):\n",
                "    asjson = json.loads(definitionjson)\n",
                "    for load in asjson['parts']:\n",
                "        load['payload'] = tobase64(load['payload'])\n",
                "    return asjson\n",
                "\n",
                "def ReplaceGUIDs(defnstring):\n",
                "    jsonstring = defnstring # json.dumps(defnstring)\n",
                "    for guid1 in newids.keys():\n",
                "        jsonstring = jsonstring.replace(guid1, newids[guid1])\n",
                "    return jsonstring\n",
                "\n",
                "\n",
                "# Create Item Function\n",
                "\n",
                "def CreateItemFromDefinition(wsid, itemname, itemtype, itemdefinition):\n",
                "    access_token = notebookutils.credentials.getToken(\"pbi\")\n",
                "    headers = {\"Authorization\": f\"Bearer {access_token}\",\n",
                "                \"Content-Type\": \"application/json\"}\n",
                "    workspaceId = wsid     \n",
                "    url = f\"https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}/items\"\n",
                "    body = {\n",
                "        \"displayName\": itemname, \n",
                "        \"type\": itemtype, \n",
                "        \"definition\": itemdefinition\n",
                "     }  \n",
                "    response = requests.post(url, headers=headers, json = body)\n",
                "    # return response.json()\n",
                "    return response"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "781d3256-5e26-41f1-bc8f-3096880bd16c"
        },
        {
            "cell_type": "code",
            "source": [
                "deploylist =  [\n",
                "    'Notebook_Drop_Create_Sales_Table_GUID'\n",
                "    ,'Dataflow_Append_Sales_Table_GUID'\n",
                "    ,'Dataflow_Replace_DIM_Tables_GUID'\n",
                "    ,'DataPipeline_Generate_Data_GUID'\n",
                "    ,'SemanticModel_DF_Market_SM_SE_GUID'\n",
                "    ,'SemanticModel_DF_Market_SM_OL_GUID'\n",
                "    ,'Report_DF_Market_Report_SE_GUID'\n",
                "    ,'Report_DF_Market_Report_OL_GUID'\n",
                "]\n",
                "\n",
                "access_token = notebookutils.credentials.getToken(\"pbi\")\n",
                "headers = {\"Authorization\": f\"Bearer {access_token}\",\n",
                "            \"Content-Type\": \"application/json\"}\n",
                "\n",
                "for replacestring in deploylist:\n",
                "    itemrecord = deploy_df[deploy_df['ReplaceString'] == replacestring]\n",
                "    definitionstring = itemrecord.iloc[0]['Definition']\n",
                "    convertedstring = convertpayloadstobase64(ReplaceGUIDs(definitionstring))\n",
                "    createitem = CreateItemFromDefinition(thisworkspaceid, itemrecord.iloc[0]['displayName'], itemrecord.iloc[0]['type'], convertedstring)\n",
                "    # createitem = CreateItemFromDefinition(thisworkspaceid, 'SMtest', itemrecord.iloc[0]['type'], convertedstring) # for troubleshooting\n",
                "\n",
                "    print(createitem.status_code)\n",
                "\n",
                "    if createitem.status_code in { 200, 201 }:\n",
                "        newitemid = createitem.json()['id']\n",
                "        newids[replacestring] = newitemid\n",
                "        print(replacestring + \" - \" + newitemid)\n",
                "\n",
                "    elif createitem.status_code==202:\n",
                "        while True:\n",
                "            url = createitem.headers[\"Location\"]\n",
                "            retry_after = createitem.headers.get(\"Retry-After\",0)\n",
                "            time.sleep(int(retry_after))\n",
                "\n",
                "            headers = {\"Authorization\": f\"Bearer {access_token}\" }\n",
                "            createitem = requests.get(url, headers=headers)\n",
                "            createitem.raise_for_status()\n",
                "\n",
                "            body = createitem.json()\n",
                "            status = body[\"status\"]\n",
                "            if status == \"Succeeded\":\n",
                "                url = createitem.headers[\"Location\"]\n",
                "                createitem = requests.get(url,headers=headers)\n",
                "                newitemid = createitem.json()['id']\n",
                "                newids[replacestring] = newitemid\n",
                "                print(replacestring + \" - \" + newitemid)\n",
                "                break"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "6655d042-8282-48c6-9f0c-ac143a02752b"
        },
        "dependencies": {}
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
