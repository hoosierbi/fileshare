{"cells":[{"cell_type":"markdown","source":["# <span style=\"color: blue;\">**DF Market** - _A Fabric Playtaset_ </span>\n","\n","## About the solution\n","- This notebook will deploy multiple related Fabric items that can be used to generate sample data of variable size for a fictional grocery store chain called \"DF Market\" (DF for Data Factory)\n","- This solution intentionally uses multiple Fabric items so that it may be useful both to demo/test Fabric workloads and/or to demo/test data analysis/visualization scenarios\n","- The resulting model has 4 dimension tables (Stores, Products, Date, and Time) and one fact table called Sales\n","- Seed values are used in the solution so that the same data will be generated each time for the same input values.\n","- The data has some useful patterns/fields in it:\n","    - one store in each city but different sales volume that change at different rates over time\n","    - sales at each hour differ by day of the week\n","    - stores have open/close dates and products have launch/discontinue dates\n","    - cities have lat/long for mapping\n","    - sales data has 1+ products per transaction for distinct count scenarios\n","    - stores table has store manager email for RLS and/or report bursting scenarios\n","    - the Stores, Products, and Date table are created last and are filtered to only include rows that exist in the Sales table\n","\n","## How to Deploy\n","- Hit \"Run All\" above or run each cell ***in order*** \n","- Once all items are deployed\n","    - Open each of the two Dataflows (append sales and replace dims) and click on \"Manage Connections\" and choose/create a Lakehouse connection. ***Save each Dataflow.*** This updates the connection and since there is a change, the save action published the dataflow. If any issues, make a small change (e.g., add a space in the formula bar) and save again to force a publish.\n","    - Open the Generate_Data pipeline and review the pipeline Variables (starting month and number of months), also review the Dataflow parameters in the ForEach with the Append Monthly Sales dataflow activity.\n","    - With the default settings, about 200 million Sales table rows will be generated in about 25 min. **Note this gets very big fast, so don't max out the values!** Do the math. For example, if you double the number of stores and double the transactions per day per store, you should get about 4X more.\n","    - Refresh the semantic model in the Workspace UI and view the report to see how many rows were created\n","\n","## Future Plans\n","- This is V1 and more is planned (e.g., incremental refresh, copy jobs to load data to WH and Eventhouse, materialized lake view aggregation tables w/ updated semantic model)\n","- Your feedback and ideas are welcomed. Please also share any additional items you create that may also be useful to others.\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b85f5796-2251-4ba1-aca8-a37f559ca989"},{"cell_type":"code","source":["import requests\n","import pandas as pd\n","import sempy.fabric as fabric\n","from sempy.fabric.exceptions import FabricHTTPException, WorkspaceNotFoundException\n","import requests\n","import base64\n","import json\n","import time\n","try:\n","    import sempy_labs as labs\n","    print('labs already installed')\n","except:\n","    print('installing labs')\n","    %pip install semantic-link-labs\n","    import sempy_labs as labs"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3b6fe170-cde5-4e91-a993-6a70d8ebc046"},{"cell_type":"code","source":["newids = {\n","    'Workspace_DFMarket_GUID': '',\n","    'Lakehouse_DF_Market_LH_GUID': '',\n","    'Report_DF_Market_Report_GUID': '', \n","    'SemanticModel_DF_Market_SM_GUID': '',\n","    'Notebook_Drop_Create_Sales_Table_GUID': '', \n","    'Dataflow_Append_Sales_Table_GUID': '', \n","    'Dataflow_Replace_DIM_Tables_GUID': '', \n","    'DataPipeline_Generate_Data_GUID': '' ,\n","    'Lakehouse_DF_Market_LH_SQLEndpoint': '',\n","    'Lakehouse_DF_Market_LH_DatabaseId': ''\n","}\n","thisworkspaceid = spark.conf.get(\"trident.workspace.id\")\n","newids['Workspace_DFMarket_GUID'] = thisworkspaceid\n","\n","newids"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a16c479f-0618-4bfb-bf1c-80425d392be4"},{"cell_type":"code","source":["# Create DF Market Lakehouse\n","access_token = notebookutils.credentials.getToken(\"pbi\")\n","headers = {\"Authorization\": f\"Bearer {access_token}\",\n","            \"Content-Type\": \"application/json\"}\n","url = f\"https://api.fabric.microsoft.com/v1/workspaces/{thisworkspaceid}/lakehouses\"\n","body = {\n","  \"displayName\": \"DF_Market_LH\",\n","  \"description\": \"DF Market Lakehouse\"\n","}\n","response = requests.post(url, headers=headers, json=body)\n","jsonresponse = response.json()\n","print(jsonresponse)\n","lakehouseid = jsonresponse['id']\n","\n","# Add new LH id to newids\n","newids['Lakehouse_DF_Market_LH_GUID'] = lakehouseid\n","newids"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"337d9078-6da7-42ce-8a6c-74ba4e7176d1"},{"cell_type":"code","source":["# Get Lakehouse SQL Endpoint\n","time.sleep(30) #gives time to create lakehouse and sql endpoint if \"Run All\" is used. Comment it out if you run each cell manually and repeat it until you see sqlendpoint and databaseid values in the output.\n","access_token = notebookutils.credentials.getToken(\"pbi\")\n","headers = {\"Authorization\": f\"Bearer {access_token}\",\n","            \"Content-Type\": \"application/json\"}\n","url = f\"https://api.fabric.microsoft.com/v1/workspaces/{thisworkspaceid}/lakehouses/{lakehouseid}\"\n","\n","response = requests.get(url, headers=headers)\n","jsonresponse = response.json()\n","# print(jsonresponse)\n","\n","# Add new LH info to newids\n","newids['Lakehouse_DF_Market_LH_SQLEndpoint'] = jsonresponse['properties']['sqlEndpointProperties']['connectionString']\n","newids['Lakehouse_DF_Market_LH_DatabaseId'] = jsonresponse['properties']['sqlEndpointProperties']['id']\n","newids"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"28d8aeda-311a-4581-8af1-1c76fb22cdec"},{"cell_type":"code","source":["# for troubleshooting\n","# lakehouseid = 'f80140a9-881b-4437-86fc-39c8baf87aef'\n","# thisworkspaceid = '17302819-7995-4a37-9d9f-86e2c5d2b2c3'"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"editable":false,"run_control":{"frozen":true}},"id":"217de458-0c7e-4fa3-9017-22920833f683"},{"cell_type":"code","source":["url = \"https://raw.githubusercontent.com/hoosierbi/fileshare/refs/heads/main/DFMarketFiles/V1/DFMarket_V1.json\"\n","deployjson = requests.get(url).text\n","deploy_df = pd.read_json(deployjson)\n","deploy_df['ReplaceString'] = deploy_df['type'] + '_' + deploy_df['displayName'].replace(' ', '_') + '_GUID'\n","deploy_df\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2c0e87b3-f2f0-48e0-a48b-976f9bcbfaf6"},{"cell_type":"code","source":["# Define Functions\n","def tobase64(textstring):\n","    textstring_bytes = textstring.encode(\"ascii\")\n","    ascii_bytes = base64.b64encode(textstring_bytes)\n","    base64_output = ascii_bytes.decode(\"ascii\")\n","    return base64_output\n","\n","def convertpayloadstobase64(definitionjson):\n","    asjson = json.loads(definitionjson)\n","    for load in asjson['parts']:\n","        load['payload'] = tobase64(load['payload'])\n","    return asjson\n","\n","def ReplaceGUIDs(defnstring):\n","    jsonstring = defnstring # json.dumps(defnstring)\n","    for guid1 in newids.keys():\n","        jsonstring = jsonstring.replace(guid1, newids[guid1])\n","    return jsonstring\n","\n","\n","# Create Item Function\n","\n","def CreateItemFromDefinition(wsid, itemname, itemtype, itemdefinition):\n","    access_token = notebookutils.credentials.getToken(\"pbi\")\n","    headers = {\"Authorization\": f\"Bearer {access_token}\",\n","                \"Content-Type\": \"application/json\"}\n","    workspaceId = wsid     \n","    url = f\"https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}/items\"\n","    body = {\n","        \"displayName\": itemname, \n","        \"type\": itemtype, \n","        \"definition\": itemdefinition\n","     }  \n","    response = requests.post(url, headers=headers, json = body)\n","    # return response.json()\n","    return response"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"781d3256-5e26-41f1-bc8f-3096880bd16c"},{"cell_type":"code","source":["deploylist =  [\n","    'Notebook_Drop_Create_Sales_Table_GUID'\n","    ,'Dataflow_Append_Sales_Table_GUID'\n","    ,'Dataflow_Replace_DIM_Tables_GUID'\n","    ,'DataPipeline_Generate_Data_GUID'\n","    ,'SemanticModel_DF_Market_SM_GUID'\n","    ,'Report_DF_Market_Report_GUID'\n","]\n","\n","access_token = notebookutils.credentials.getToken(\"pbi\")\n","headers = {\"Authorization\": f\"Bearer {access_token}\",\n","            \"Content-Type\": \"application/json\"}\n","\n","for replacestring in deploylist:\n","    itemrecord = deploy_df[deploy_df['ReplaceString'] == replacestring]\n","    definitionstring = itemrecord.iloc[0]['Definition']\n","    convertedstring = convertpayloadstobase64(ReplaceGUIDs(definitionstring))\n","    createitem = CreateItemFromDefinition(thisworkspaceid, itemrecord.iloc[0]['displayName'], itemrecord.iloc[0]['type'], convertedstring)\n","    # createitem = CreateItemFromDefinition(thisworkspaceid, 'SMtest', itemrecord.iloc[0]['type'], convertedstring) # for troubleshooting\n","\n","    print(createitem.status_code)\n","\n","    if createitem.status_code in { 200, 201 }:\n","        newitemid = createitem.json()['id']\n","        newids[replacestring] = newitemid\n","        print(replacestring + \" - \" + newitemid)\n","\n","    elif createitem.status_code==202:\n","        while True:\n","            url = createitem.headers[\"Location\"]\n","            retry_after = createitem.headers.get(\"Retry-After\",0)\n","            time.sleep(int(retry_after))\n","\n","            headers = {\"Authorization\": f\"Bearer {access_token}\" }\n","            createitem = requests.get(url, headers=headers)\n","            createitem.raise_for_status()\n","\n","            body = createitem.json()\n","            status = body[\"status\"]\n","            if status == \"Succeeded\":\n","                url = createitem.headers[\"Location\"]\n","                createitem = requests.get(url,headers=headers)\n","                newitemid = createitem.json()['id']\n","                newids[replacestring] = newitemid\n","                print(replacestring + \" - \" + newitemid)\n","                break"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6655d042-8282-48c6-9f0c-ac143a02752b"},{"cell_type":"code","source":["#bind semantic model to new lakehouse\n","from sempy_labs.directlake import update_direct_lake_model_lakehouse_connection\n","dataset_name = \"DF_Market_SM\"\n","lakehouse_name = \"DF_Market_LH\"\n","update_direct_lake_model_lakehouse_connection(dataset=dataset_name,lakehouse=lakehouse_name)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f76ecacb-f4ae-4d7f-86da-c33561dd24e2"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}